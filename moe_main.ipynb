{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Basic MOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicExpert(nn.Module): \n",
    "    def __init__(self, feature_in, feature_out): \n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(feature_in, feature_out)\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.fc(x)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "class BasicMOE(nn.Module): \n",
    "    def __init__(self, feature_in, feature_out, num_experts): \n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(feature_in,  num_experts)\n",
    "        # output shape: (batch_size, num_experts)\n",
    "        self.experts = nn.ModuleList(\n",
    "            BasicExpert(\n",
    "                feature_in, feature_out\n",
    "            ) for _ in range(num_experts)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        # x shape is (batch, feature_in)\n",
    "        # feature_in / hidden_size hidden_dim\n",
    "        expert_weights = self.gate(x)\n",
    "        expert_out_list = [\n",
    "            expert(x) for expert in self.experts\n",
    "        ] # each expert will output a shape of (batch_size, feature_out)\n",
    "\n",
    "        expert_outputs = [\n",
    "            expert_out.unsqueeze(1)\n",
    "            for expert_out in expert_out_list\n",
    "        ] \n",
    "\n",
    "        # expert out is (b, 1, feature_out)\n",
    "        expert_output = torch.concat(\n",
    "            expert_outputs, \n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "\n",
    "        # expert_weights\n",
    "        expert_weights = F.softmax(expert_weights, dim=1)\n",
    "        # expert_weights shape (batch_size, num_experts)\n",
    "\n",
    "        expert_weights = expert_weights.unsqueeze(1)\n",
    "\n",
    "        # expected output shape (batch_size, feature_out)\n",
    "\n",
    "        output = expert_weights @ expert_output\n",
    "        return output.squeeze(1)\n",
    "    \n",
    "\n",
    "def test_basic_moe(): \n",
    "    x = torch.rand(4, 512)\n",
    "    basic_moe = BasicMOE(512, 128, 4)\n",
    "    output = basic_moe(x)\n",
    "    print(output.shape)\n",
    "\n",
    "test_basic_moe()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Sparse MOE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse MOE would pick top k experts, and weights the outputs from these k experts. Tokens will be processed by ALL of the k experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16]) torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "# mistral MOE\n",
    "\n",
    "class MOEConfig: \n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_dim, \n",
    "            expert_number, \n",
    "            top_k, \n",
    "            shared_experts_number=2): \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.expert_number = expert_number\n",
    "        self.top_k = top_k\n",
    "        self.shared_experts_number = shared_experts_number\n",
    "\n",
    "    \n",
    "class MOERouter(nn.Module): \n",
    "    def __init__(self, config): \n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(config.hidden_dim, config.expert_number)\n",
    "        # but only choose top k experts\n",
    "\n",
    "        self.expert_number = config.expert_number\n",
    "        self.top_k = config.top_k\n",
    "\n",
    "    def forward(self, x): \n",
    "        # E.g., expert_num = 8; k = 2\n",
    "        router_logits = self.gate(x) # (batch * seq_len, expert_number)\n",
    "\n",
    "        # calculate the possibility of each expert\n",
    "        router_probs = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "\n",
    "        # calcualte top_k experts outputs\n",
    "        # weight and the index/position of the expert\n",
    "        # topk can be backpropagated\n",
    "        router_weights, selected_experts_indices = torch.topk(\n",
    "            input=router_probs,\n",
    "            k=self.top_k,\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        # shape of router_weights and selected_experts_indices: \n",
    "        # (batch * seq_len, top_k)\n",
    "\n",
    "        # normalization\n",
    "        router_weights = router_weights / router_weights.sum(\n",
    "            dim=-1, \n",
    "            keepdim=True\n",
    "        )\n",
    "        router_weights = router_weights.to(x.dtype)\n",
    "\n",
    "        expert_masks = F.one_hot(\n",
    "            selected_experts_indices,\n",
    "            num_classes=self.expert_number\n",
    "        ) # (batch * seq_len, top_k, expert_num)\n",
    "\n",
    "        expert_masks = expert_masks.permute(2, 1, 0)\n",
    "        # (expert_num, top_k, batch*seq_len)\n",
    "\n",
    "        return router_logits, router_weights, selected_experts_indices, expert_masks\n",
    "        # router_logits: (batch*seq_len, expert_num)\n",
    "        # router_weights: (batch*seq_len, top_k)\n",
    "        # selected_expert_indices: (batch*seq_len, top_k)\n",
    "        # expert_mask: (expert_number, top_k, batch * seq_len)\n",
    "\n",
    "\n",
    "\n",
    "class SparseMOE(nn.Module): \n",
    "    def __init__(self, config): \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.expert_number = config.expert_number\n",
    "        self.top_k = config.top_k\n",
    "\n",
    "        # init experts\n",
    "        self.experts = nn.ModuleList(\n",
    "            BasicExpert(\n",
    "                config.hidden_dim, \n",
    "                config.hidden_dim, \n",
    "            ) for _ in range(config.expert_number)\n",
    "        )\n",
    "        self.router = MOERouter(config)\n",
    "\n",
    "    def forward(self, x): \n",
    "        # x shape: (batch, seq_len, hidden_dim)\n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "\n",
    "        # token dimension calculation\n",
    "        # reshape x (batch * seq_len, hidden_dim)\n",
    "        hidden_states = x.view(-1, hidden_dim)\n",
    "\n",
    "        # expert calculation\n",
    "        router_logits, router_weights, selected_experts_indices, expert_masks = self.router(hidden_states)\n",
    "        # expert_mask shape: (exper_num, top_k, batch*seq_len)\n",
    "        # selected_experts_indices shape: (batch_size * seq_len, top_k)\n",
    "        \n",
    "        # the final hidden_states shape: (batch*seq_len, hidden_dim)\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * seq_len, hidden_dim), \n",
    "            dtype=hidden_states.dtype,\n",
    "            device=hidden_states.device\n",
    "        )\n",
    "\n",
    "        # retrieve each expert\n",
    "        # add the hidden_states of the expert's token to final_hidden_states\n",
    "\n",
    "        for expert_idx in range(self.expert_number): \n",
    "            expert_layer = self.experts[expert_idx]\n",
    "\n",
    "            current_expert_mask = expert_masks[expert_idx]\n",
    "\n",
    "            router_weights_idx, top_x = torch.where(current_expert_mask)\n",
    "            # router_weights_idx is the index of the expert  \n",
    "            # top_x is the index of the token in the batch * seq_len\n",
    "\n",
    "            # router_weights_idx is used to pick the weight\n",
    "            # top_x is used to pick the hidden_states\n",
    "            # they are both one-dim values\n",
    "\n",
    "            current_state = hidden_states.unsqueeze(0)[:, top_x, :].reshape(-1, hidden_dim)\n",
    "\n",
    "            current_state = expert_layer(current_state)\n",
    "\n",
    "            # hidden_states shape: (batch * seq_len, hidden_dim)\n",
    "            # hidden_states.unsequeeze(0) shape: (1, batch * seq_len, hidden_dim)\n",
    "            # current_state shape: (selected_token_num, hidden_dim)\n",
    "\n",
    "            current_token_router_weight = router_weights[top_x, router_weights_idx]\n",
    "            # router_weight shape: (batch_size * seq_len, top_k)\n",
    "            # current_token_router_weight shape: (selected_token_number)\n",
    "\n",
    "            current_token_router_weight = current_token_router_weight.unsqueeze(-1)\n",
    "            # current_token_router_weight shape: (selected_token_number, 1)\n",
    "\n",
    "            current_hidden_states = current_state * current_token_router_weight\n",
    "            # curret_state shape:  (selected_token_num, hidden_dim)\n",
    "            # current_token_router_weight shape: (selected_token_number, 1)\n",
    "\n",
    "            final_hidden_states.index_add (\n",
    "                0, \n",
    "                top_x, \n",
    "                current_hidden_states.to(hidden_states.dtype)\n",
    "            )\n",
    "\n",
    "        # revert the final_hidden_states to original shape\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        return final_hidden_states\n",
    "\n",
    "\n",
    "def test_token_level_moe(): \n",
    "    x = torch.rand(2, 4, 16)\n",
    "    config = MOEConfig(16, 2, 2)\n",
    "    token_level_moe = SparseMOE(config)\n",
    "    out = token_level_moe(x)\n",
    "    print(out[0].shape, out[1].shape)\n",
    "\n",
    "test_token_level_moe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. ShareExpert SparseMOE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare to the SparseMOE, SESMOE has shared expert models, which all the token would be passed to the shared experts, and each token will pick the top k experts with the calculated Router weights. Finally, combine the output from top k experts and shared experts, weight them and summerize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16]) torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "class SharedExpertMOE(nn.Module): \n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.router_experts_moe = SparseMOE(config)\n",
    "        self.shared_experts = nn.ModuleList(\n",
    "            [\n",
    "                BasicExpert(self.config.hidden_dim, self.config.hidden_dim)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        # x shape: (batch_size, seq_len, hidden_dim)\n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "\n",
    "        shared_experts_output_list = [\n",
    "            expert(x) for expert in self.shared_experts\n",
    "        ]\n",
    "\n",
    "        shared_expert_output = torch.stack(\n",
    "            shared_experts_output_list, \n",
    "            dim=0\n",
    "        )\n",
    "        # shape (shared_experts_num, batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        shared_expert_output = shared_expert_output.sum(dim=0, keepdim=False)\n",
    "        # shape (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        sparse_moe_out, router_logits = self.router_experts_moe(\n",
    "            x\n",
    "        )\n",
    "\n",
    "        output = shared_expert_output + sparse_moe_out\n",
    "\n",
    "        return output, router_logits\n",
    "\n",
    "\n",
    "def test_share_expert_moe():\n",
    "    x = torch.rand(2, 4, 16)\n",
    "    config = MOEConfig(16, 2, 2)\n",
    "    share_expert_moe = SharedExpertMOE(config)\n",
    "    out = share_expert_moe(x)\n",
    "    print(out[0].shape, out[1].shape)\n",
    "\n",
    "\n",
    "test_share_expert_moe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_load_balancing_loss(router_logits: torch.Tensor, num_experts: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        router_logits: shape [batch_size * sequence_length, num_experts]\n",
    "        num_experts\n",
    "    \n",
    "    Returns:\n",
    "        total_loss: auxiliary_loss + z_loss\n",
    "    \"\"\"\n",
    "    \n",
    "    router_probs = torch.softmax(router_logits, dim=-1)  # [b*s, num_experts]\n",
    "    \n",
    "   \n",
    "    _, selected_experts = torch.topk(router_probs, k=2, dim=-1)  # [b*s]\n",
    "    \n",
    "\n",
    "    mask = torch.nn.functional.one_hot(selected_experts, num_experts).float()  # [b*s, num_experts]\n",
    "    \n",
    "    \n",
    "    expected_load = torch.ones_like(router_probs) / num_experts\n",
    "    \n",
    "   \n",
    "    actual_load = mask.mean(dim=0)  # [num_experts]\n",
    "    \n",
    " \n",
    "    aux_loss = torch.sum(actual_load * router_probs.mean(dim=0)) * num_experts\n",
    "    \n",
    "  \n",
    "    z_loss = torch.mean(torch.square(router_logits))\n",
    "    z_loss_weight = 0.001  \n",
    "    \n",
    "    \n",
    "    total_loss = aux_loss + z_loss * z_loss_weight\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def test_moe_training():\n",
    "    # Create a simple dataset\n",
    "    batch_size = 32\n",
    "    seq_len = 16\n",
    "    hidden_dim = 32\n",
    "    num_batches = 100\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    config = MOEConfig(hidden_dim=hidden_dim, \n",
    "                      expert_number=4,\n",
    "                      top_k=2,\n",
    "                      shared_experts_number=2)\n",
    "    model = SharedExpertMOE(config)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for batch in range(num_batches):\n",
    "        # Generate random input data\n",
    "        x = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "        target = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Forward pass\n",
    "        output, router_logits = model(x)\n",
    "\n",
    "        # Compute losses\n",
    "        # MSE loss for prediction\n",
    "        mse_loss = F.mse_loss(output, target)\n",
    "        \n",
    "        aux_loss = switch_load_balancing_loss(router_logits, config.expert_number)\n",
    "        # Combined loss\n",
    "        total_loss = mse_loss + 0.01 * aux_loss\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            print(f\"Batch {batch}, Loss: {total_loss.item():.4f} \"\n",
    "                  f\"(MSE: {mse_loss.item():.4f}, Aux: {aux_loss.item():.4f})\")\n",
    "\n",
    "# Run the training test\n",
    "test_moe_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len, hidden_dim = a.size()\n",
    "batch_size, seq_len, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8919, 0.8194, 0.3395],\n",
       "         [0.8881, 0.0511, 0.7392]],\n",
       "\n",
       "        [[0.1639, 0.6840, 0.3140],\n",
       "         [0.5633, 0.6653, 0.5879]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0508, 1.6783],\n",
       "        [1.1619, 1.8164]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_sum = a.sum(dim=-1, keepdim=False)\n",
    "a_sum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
